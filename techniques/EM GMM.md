- Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)
- tries to fix the simplicity of kmeans by naively using the mean value for the cluster center
    - so kmeans fails with the case with one ring inside another
    - I think that kmeans works much better when they're circular
- We assume that the points are gaussian distributed in GMMs
    - so we have two variables to describe the shape of the clusters, the mean point and the standard deviation!
    - in two-dimensions, the clusters can have an elliptical shape
        - aka our clusters are gaussian
- Expectation–Maximization is an optimization algorithm to find the gaussian parameters for each cluster

- steps
    - 1) select the number of clusters (like kmeans) and randomly initialize the gaussian data for each cluster
    - 2) for each gaussian dist, compute the probability that the datapoint belongs to the cluster. These are the weights
        - points closer to the center of the gaussian will have a higher probability
        - notice that this step requires knowledge of the center of the cluster and the standard deviation of the cluster
    - 3) now we will compute the new variables that will maximize the probability of every point being in the gaussian.
        - we calculate the mean by the weighted sum of the point positions. We use the weights calculated in point 2.
            - This shifts the cluster
        - Then we recalculate the standard deviation
    - 4) repeat steps 2 & 3 until convergence (the distributions don't change much at each iteration)
- pros
    - a lot more flexible in terms of cluster convergence than [[kmeans]]
    - the standard deviation param means that each cluster can be eclipses, instead of just circles
    - GMMs also use probabilities, so they can have multiple clusters per datapoint (this is mixed membership)